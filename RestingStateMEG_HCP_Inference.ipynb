{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:107: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import scipy.optimize as spo\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import numpy.linalg as npl\n",
    "import numpy.random as npr\n",
    "from scipy.stats.mstats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "root2over = 1/np.sqrt(2)\n",
    "erf_max = sp.special.erf(root2over)\n",
    "\n",
    "#list of patient ids that are associated with the preprocessed MEG files from HCP. HCP database has 96 total\n",
    "dataset_ids = ['100307', '102816', '105923', '106521', '108323', '109123', '111514', '112920', '116524', '116726',\n",
    "               '133019', '140117', '146129', '149741', '153732', '154532', '156334', '158136', '162026', '162935',\n",
    "               '164636', '166438', '169040', '172029', '174841', '175237', '175540', '177746', '179245', '181232',\n",
    "               '185442', '187547', '189349', '191033', '191437', '191841', '192641', '195041', '198653', '204521', \n",
    "               '205119', '212318', '212823', '214524', '221319', '223929', '233326', '248339', '250427', '255639',\n",
    "               '257845', '283543', '287248', '293748', '352132', '352738', '353740', '358144', '406836', '433839', \n",
    "               '512835', '555348', '559053', '568963', '581450', '599671', '601127', '660951', '662551', '665254', \n",
    "               '667056', '679770', '680957', '706040', '707749', '715950', '725751', '735148', '783462', '814649',\n",
    "               '825048', '872764', '877168', '891667', '898176', '912447', '917255', '990366']\n",
    "\n",
    "#function to standardize the MEG data matrices, which have very small values around e-13, e-14\n",
    "def standardize(data_matrix):\n",
    "    #create a copy of the data_matrix and make it normalized_sensor_matrix, will just replace values in row during loop\n",
    "    x = data_matrix[:]\n",
    "    x-=np.mean(x) # the -= means can be read as x = x- np.mean(x)\n",
    "    x/=np.std(x) # the /= means can be read as x = x/np.std(x)\n",
    "    return x\n",
    "\n",
    "# helper function to store all the sensor labels across patients present in the total dataset\n",
    "def store_labels(sensor_labels, patient):\n",
    "    all_sensor_labels = []\n",
    "    for i in range(sensor_labels.shape[0]):\n",
    "        new_tag = sensor_labels[i][0][0]+\"_\"+patient\n",
    "        all_sensor_labels.append(new_tag)\n",
    "    return all_sensor_labels\n",
    "\n",
    "def store_reg_labels(sensor_labels, patient):\n",
    "    all_sensor_labels = []\n",
    "    for i in range(sensor_labels.shape[0]):\n",
    "        new_tag = sensor_labels[i][0][0]\n",
    "        all_sensor_labels.append(new_tag)\n",
    "    return all_sensor_labels\n",
    "\n",
    "#function to load the MEG data from an array with all the patient ids. Returns the raw and normalized MEG data\n",
    "def load_sensor_matrix(id_list):\n",
    "    labels_dict = {}\n",
    "    separated_dataset = {}\n",
    "    first_iteration = True   #boolean value that accomodates the first run through of MEG data\n",
    "    for patient in id_list:\n",
    "        data = scipy.io.loadmat('Documents/HCP_dataset/'+patient+'/MEG/Restin/rmegpreproc/'+patient+'_MEG_3-Restin_rmegpreproc.mat')\n",
    "        data_times = data['data'][0][0][5][:]    #extracts the timecourses of the MEG data\n",
    "        data_labels = data['data'][0][0][6][:]  #extracts the sensor labels for the MEG data\n",
    "        data_trials = data['data'][0][0][4][:]   #extracts the MEG potentials over the given timecourse\n",
    "        separated_dataset[patient] = data_trials[0][0]\n",
    "        if first_iteration:\n",
    "            sensor_matrix = data_trials[0][0][:]\n",
    "            first_iteration = False    #no longer in the first iteration of the loop, set to false\n",
    "        else:\n",
    "            #if not in the first run through, append the matrices together, to form a combined matrix\n",
    "            sensor_matrix = np.concatenate((sensor_matrix, data_trials[0][0][:]), axis = 0)\n",
    "        #create an array with all the sensor_labels for every patient in the dataset\n",
    "        all_data_labels = store_labels(data_labels, patient)\n",
    "        labels_dict[patient] = set(store_reg_labels(data_labels, patient))\n",
    "    return sensor_matrix, separated_dataset, all_data_labels, labels_dict\n",
    "\n",
    "def common_labels(labels_dict):\n",
    "    s = set()\n",
    "    labels_dict_values = list(labels_dict.values())\n",
    "    for index in range(len(labels_dict_values)-1):\n",
    "        if bool(s) == False:\n",
    "            s = set(labels_dict_values[index]).intersection(labels_dict_values[index+1])\n",
    "        else:\n",
    "            s = s.intersection(set(labels_dict_values[index+1]))\n",
    "    return s\n",
    "\n",
    "def form_dataset(common_labels_set, separated_dataset, labels_dict):\n",
    "    streamlined_dataset = {}\n",
    "    for patient in separated_dataset:\n",
    "        for label in common_labels_set:\n",
    "            channel_number = list(labels_dict[patient]).index(label)\n",
    "            if patient not in streamlined_dataset:\n",
    "                streamlined_dataset[patient] = [separated_dataset[patient][channel_number, :]]\n",
    "            else:\n",
    "                streamlined_dataset[patient].append(separated_dataset[patient][channel_number, :])\n",
    "    return streamlined_dataset\n",
    "\n",
    "def find_sigma(y,h):\n",
    "    time_steps,size = y.shape\n",
    "    sigma = np.std(y-h,axis=0)\n",
    "    #     sigma = np.random.rand(1,size) + 0.5\n",
    "    #     for index in range(size):\n",
    "    #         def f0(sig):\n",
    "    #             return (1-np.std(y[:,index]/np.abs(sig) - h[:,index]))**2\n",
    "    #         res = spo.minimize(f0,sigma[0,index])\n",
    "    #         sigma[0,index] = np.abs(res.x)\n",
    "    return(sigma.reshape(1,size))\n",
    "\n",
    "def time_shift_cov(x,shift=1):\n",
    "    time_steps,size = x.shape\n",
    "    x0 = (x - np.mean(x,axis=0))/np.std(x,axis=0)\n",
    "    return x0[shift:].T.dot(x0[:-shift])/np.float(time_steps-shift)\n",
    "\n",
    "def enet_solve(c,b):\n",
    "    regr = ElasticNet(random_state=0,max_iter=10000)\n",
    "    regr.fit(c,b)\n",
    "    return regr.coef_\n",
    "\n",
    "def moving_avg(a,window):\n",
    "    cu = np.cumsum(a,axis=0,dtype=float)\n",
    "    cu[window:] = cu[window:] - cu[:-window]\n",
    "    return cu[window-1:]/window\n",
    "\n",
    "def odd_power(h,power=3):\n",
    "    sign = np.sign(h)\n",
    "    return sign*np.power(np.abs(h),1/power)\n",
    "\n",
    "#Now add in a bias\n",
    "def simulate(size,time_steps,w = 0.0,sigma = 1.0,coupling = 1.0,rho = 0.2,power=1,delta = 1,bias=0):\n",
    "    if npl.norm(w)==0.:\n",
    "        sigma = (npr.rand(1,size)+1)/2.0        \n",
    "        if power<3: delta = (npr.rand(1,size)+1)*4\n",
    "        else: delta = odd_power(npr.rand(1,size)+1,power)        \n",
    "        bias = (npr.rand(1,size)-0.5)*2\n",
    "        w = npr.rand(size,size) - 0.5\n",
    "        w = coupling*w - rho*np.eye(size)\n",
    "    x = np.zeros((time_steps,size))\n",
    "    x_min = np.zeros((1,size))\n",
    "    x[0] = npr.rand(1,size)-0.5\n",
    "    for i in range(time_steps-1):\n",
    "        if power<3:\n",
    "            x[i+1] = (1)*x[i] + delta * np.tanh(bias + x[i].dot(w)) + sigma*npr.normal(size=(1,size))\n",
    "        else:\n",
    "            x[i+1] = (1)*x[i] + delta * odd_power(bias + x[i].dot(w),power) + sigma*npr.normal(size=(1,size))\n",
    "    y = np.diff(x,axis=0)\n",
    "    y_max = np.max(np.abs(y),axis=0)\n",
    "    if power<3:\n",
    "        return x,y_max[:,None]*w,sigma/y_max[None,:],bias\n",
    "    else:\n",
    "        opd = odd_power(delta,1/power)\n",
    "        return x,opd[0][None,:]*w,sigma,opd*bias\n",
    "\n",
    "def bias_update(y,h,b_in,pp):\n",
    "    y_plus = y>0\n",
    "    if pp==1:\n",
    "        def f0(bias):\n",
    "            return np.mean(y[y_plus]-np.tanh(bias+h[y_plus]))**2 + np.mean(y[~y_plus]-np.tanh(bias+h[~y_plus]))**2\n",
    "    else:\n",
    "        def f0(bias):\n",
    "            return np.mean(y[y_plus]-odd_power(bias + h[y_plus],pp))**2 + \\\n",
    "                np.mean(y[~y_plus]-odd_power(bias + h[~y_plus],pp))**2\n",
    "    res = spo.minimize(f0,b_in)\n",
    "    return res.x\n",
    "\n",
    "\n",
    "def infer(x,max_iter = 100,tol=1e-8,func=npl.solve,window=1,power=1,verbose=False):\n",
    "    time_steps,size = x.shape\n",
    "    x0 = np.copy(x)\n",
    "    if window>1:\n",
    "        x0 = moving_avg(x0,window)\n",
    "        time_steps = time_steps-window+1\n",
    "    y = np.diff(x0,axis=0)\n",
    "    y_mean = np.mean(np.abs(y),axis=0)\n",
    "    y_max = np.max(np.abs(y),axis=0)\n",
    "    if power<3:\n",
    "        y /= y_max[None,:]#now y is definitely within +/- 1\n",
    "        x0 = x0/y_max[None,:]\n",
    "    x0 = x0[:-1]\n",
    "    s = np.sign(y)\n",
    "    c = np.cov(x0,rowvar=False)\n",
    "    w = npr.rand(size,size) - 0.5\n",
    "    bias = npr.rand(1,size) - 0.5\n",
    "    if power<3:\n",
    "        h = np.tanh(bias + x0.dot(w))\n",
    "    else:\n",
    "        h = odd_power(bias + x0.dot(w),power)\n",
    "    for index in range(size):\n",
    "        err_old,error,counter = 0,np.inf,0\n",
    "        #         print(index)\n",
    "        while np.abs(error-err_old) > tol and counter < max_iter:\n",
    "            counter += 1\n",
    "            zeros = np.abs(bias[0,index] + x0.dot(w[:,index])) < 1e-7\n",
    "            if power<3:\n",
    "                ratio = np.sqrt(np.pi/2.0)*np.ones((time_steps-1))\n",
    "            else:\n",
    "                ratio = np.sqrt(np.pi/2.0)*np.ones((time_steps-1))*h[:,index]**(power-1)\n",
    "            ratio[~zeros] = (bias[0,index] + x0[~zeros,:].dot(w[:,index]))/sp.special.erf(h[~zeros,index]*root2over)\n",
    "            w[:,index] = func(c+0.1*np.eye(size),np.mean((x0-np.mean(x0,axis=0)[None,:])*(s[:,index]*ratio)[:,np.newaxis],axis=0))\n",
    "            h_temp = x0.dot(w[:,index])\n",
    "            bias[0,index] = bias_update(y[:,index],h_temp,bias[0,index],pp=power)\n",
    "            err_old = error\n",
    "            if power<3:\n",
    "                h[:,index] = np.tanh(bias[0,index] + h_temp)\n",
    "                error = npl.norm(s[:,index]-sp.special.erf(h[:,index]*root2over)/erf_max)\n",
    "            else:\n",
    "                h[:,index] = odd_power(bias[0,index] + h_temp,power)\n",
    "                error = npl.norm(s[:,index]-sp.special.erf(h[:,index]*root2over))\n",
    "#             print(counter,error)\n",
    "    sigma = find_sigma(y,h)*np.sqrt(window)#*y_max[None,:]\n",
    "    return w,sigma,bias\n",
    "\n",
    "def pca_combine_datasets(streamlined_dataset):\n",
    "    pca_combined_dataset = np.array([])\n",
    "    for patient in streamlined_dataset:\n",
    "        x = np.asarray(streamlined_dataset[patient])\n",
    "        if pca_combined_dataset.size == 0:\n",
    "            pca_combined_dataset = x\n",
    "        else:\n",
    "            pca_combined_dataset = np.concatenate((pca_combined_dataset, x), axis=1 )\n",
    "    return pca_combined_dataset\n",
    "\n",
    "def calculate_allW(dataset):\n",
    "    allwmatrices = {}\n",
    "    allsigs = {}\n",
    "    allbiases = {}\n",
    "    for patient in dataset:\n",
    "        w, sig, bias = infer(dataset[patient].transpose(), power=3)\n",
    "        allwmatrices[patient] = w\n",
    "        allsigs[patient] = sig\n",
    "        allbiases[patient] = bias\n",
    "    return allwmatrices, allsigs, allbiases\n",
    "\n",
    "def calculate_allTimeCov(allwmatrices, time_shift):\n",
    "    alltimeshiftcov = {}\n",
    "    for patient in allwmatrices:\n",
    "        timeshiftcovmatrix = time_shift_cov(allwmatrices[patient], shift=time_shift)\n",
    "        alltimeshiftcov[patient] = timeshiftcovmatrix\n",
    "    return alltimeshiftcov\n",
    "\n",
    "def main(dataset_ids, pca_components, time_shift):\n",
    "    sensor_matrix, separated_dataset, all_data_labels, labels_dict = load_sensor_matrix(dataset_ids)\n",
    "    common_labels_set = common_labels(labels_dict)\n",
    "    streamlined_dataset = form_dataset(common_labels_set, separated_dataset, labels_dict)\n",
    "    normalized_streamlined_dataset = {}\n",
    "    for patient in streamlined_dataset:\n",
    "        normalized_streamlined_dataset[patient] = standardize(streamlined_dataset[patient])\n",
    "    pca_data_dict = {}\n",
    "    pca_combined_dataset = pca_combine_datasets(normalized_streamlined_dataset)\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    pca.fit(pca_combined_dataset.T)\n",
    "    transformed_data = pca.fit_transform(pca_combined_dataset.T)\n",
    "    counter = 0\n",
    "    for patient in normalized_streamlined_dataset:\n",
    "        pca_data_dict[patient] = transformed_data[counter*1018:1018*(counter+1), :].T\n",
    "        counter = counter + 1\n",
    "    allwmatrices, allsigs, allbiases = calculate_allW(pca_data_dict)\n",
    "    alltimeshiftcov = calculate_allTimeCov(allwmatrices, time_shift)\n",
    "    return allwmatrices, allsigs, allbiases, alltimeshiftcov\n",
    "\n",
    "time_shift=10\n",
    "pca_components=10\n",
    "allwmatrices, allsigs, allbiases, alltimeshiftcov = main(dataset_ids, pca_components, time_shift)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
